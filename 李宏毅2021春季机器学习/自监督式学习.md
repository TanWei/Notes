transformer<br/>
盖住一个token<br/>
“台湾大学” 盖住湾
分类问题，类别与中文字的数目相同，学习到被盖住的地方类别为’湾‘<br/>
### next sentence prediction
预测两个句子是否接在一起<br/>
输入【cls】w1 w2 【w3 w4 w5】<br/>
只取【cls】
没有什么用<br/>
原因：可能问题太简单了<br/>
总结：bert做了什么？<br/>
1、填空题<br/>
2、预测两个句子是否接在一起<br/>
实际做了什么？
Downstream tasks（下游任务）==》通过<font color='red'>pre——train</font><br/>
bert可以做很多意想之外的问题。<br/>
# 有：
GLUE（自然语言数据集）包含的9个任务：<br/>
怎么用？<br/>
case 1：<br/>
一个句子是正面/负面？<br/>
初始化，使用填空题的bert初始化参数。<br/>
pre-train v.s. random initilization<br/>
![图片](./howtobert.png)
case 2:<br/>
怎么用bert解决’i saw a saw‘问题，词性分析？<br/>
![图片](./howtobert2.png)
仍然需要一些资料，有标注，区别就是使用bert初始化参数。<br/>
case 3：<br/>
输入两个句子，输出一个类别<br/>
narural language infrerencee（nli）<br/>
case 4：<br/>
作业。。。<br/>
简单的问答系统，答案就在文章里面<br/>
训练bert：3billion个参数，无法自己训练<br/>


## GPT模型
预测下一个出现的词<br/>
不需要样例。<br/>
类似few-shot learning ， in-context learning<br/>
例子：<br/>
翻译中文为英文：<br/>
他=》he，攻击=》fight<br/>
准确率平均“50%”多。<br/>

